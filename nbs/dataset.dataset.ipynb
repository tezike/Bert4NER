{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bert4NER.config as config\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EntityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, pos, tags):\n",
    "        self.texts = texts\n",
    "        self.pos = pos\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        pos = self.pos[item]\n",
    "        tag = self.tags[item]\n",
    "\n",
    "        tokens = []\n",
    "        target_pos = []\n",
    "        target_tag = []\n",
    "        \n",
    "        # tokenize the each word in the text string\n",
    "        for i, word in enumerate(text):\n",
    "            inputs = config.TOKENIZER.encode(\n",
    "                word,\n",
    "                add_special_tokens = False,\n",
    "                truncation = True\n",
    "            )\n",
    "\n",
    "            input_len = len(inputs)\n",
    "            tokens.extend(inputs)\n",
    "\n",
    "            # the tag for that particular word should be the same for all the \n",
    "            # sub tokens of the word\n",
    "            \n",
    "            target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tag[i]] * input_len)\n",
    "\n",
    "        tokens = tokens[:config.MAX_SEQ_LEN - 2]\n",
    "        target_pos = target_pos[:config.MAX_SEQ_LEN - 2]\n",
    "        target_tag = target_tag[:config.MAX_SEQ_LEN - 2]\n",
    "\n",
    "        tokens = [101] + tokens + [102]\n",
    "        target_pos = [0] + target_pos + [0]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(tokens)\n",
    "        token_type_ids = [0] * len(tokens)\n",
    "\n",
    "        pad_len = (config.MAX_SEQ_LEN) - len(tokens)\n",
    "\n",
    "        tokens = tokens + ([0] * pad_len)\n",
    "        mask = mask + ([0] * pad_len)\n",
    "        token_type_ids = token_type_ids + ([0] * pad_len)\n",
    "        target_pos = target_pos + ([0] * pad_len)\n",
    "        target_tag = target_tag + ([0] * pad_len)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target_pos': torch.tensor(target_pos, dtype=torch.long),\n",
    "            'target_tag': torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "df = pd.read_csv(config.DATA_PATH/'ner_datasetreference.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the foward fill method in pandas to fill all the nans for the each sentence in the `Sentence #` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              Sentence: 1\n",
       "1              Sentence: 1\n",
       "2              Sentence: 1\n",
       "3              Sentence: 1\n",
       "4              Sentence: 1\n",
       "                ...       \n",
       "1048570    Sentence: 47959\n",
       "1048571    Sentence: 47959\n",
       "1048572    Sentence: 47959\n",
       "1048573    Sentence: 47959\n",
       "1048574    Sentence: 47959\n",
       "Name: Sentence #, Length: 1048575, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "df['Sentence #'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we cans ee that there are 47959 sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Sentence #'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us encode all the labels for every word in every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "le_pos = LabelEncoder()\n",
    "le_tag = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df[\"encoded_POS\"] = le_pos.fit_transform(df.POS)\n",
    "df[\"encoded_Tag\"] = le_tag.fit_transform(df.Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['$', ',', '.', ':', ';', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ',\n",
       "        'JJR', 'JJS', 'LRB', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT',\n",
       "        'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'RRB', 'TO', 'UH',\n",
       "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',\n",
       "        '``'], dtype=object),\n",
       " array(['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per',\n",
       "        'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org',\n",
       "        'I-per', 'I-tim', 'O'], dtype=object))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "le_pos.classes_, le_tag.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now group the df according to the `Sentence #` and use it to curate the `Word`, `POS` and `Tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_group = df.groupby('Sentence #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "grouped_words = sentence_group['Word'].apply(list)\n",
    "grouped_POS = sentence_group['encoded_POS'].apply(list)\n",
    "grouped_Tag = sentence_group['encoded_Tag'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #\n",
       "Sentence: 1        [Thousands, of, demonstrators, have, marched, ...\n",
       "Sentence: 10       [Iranian, officials, say, they, expect, to, ge...\n",
       "Sentence: 100      [Helicopter, gunships, Saturday, pounded, mili...\n",
       "Sentence: 1000     [They, left, after, a, tense, hour-long, stand...\n",
       "Sentence: 10000    [U.N., relief, coordinator, Jan, Egeland, said...\n",
       "                                         ...                        \n",
       "Sentence: 9995     [Opposition, leader, Mir, Hossein, Mousavi, ha...\n",
       "Sentence: 9996     [On, Thursday, ,, Iranian, state, media, publi...\n",
       "Sentence: 9997     [Following, Iran, 's, disputed, June, 12, elec...\n",
       "Sentence: 9998     [Since, then, ,, authorities, have, held, publ...\n",
       "Sentence: 9999     [The, United, Nations, is, praising, the, use,...\n",
       "Name: Word, Length: 47959, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = grouped_words.values\n",
    "tags = grouped_Tag.values\n",
    "pos = grouped_POS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 47959, 47959)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "len(sentences), len(tags), len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 10, 19, ..., 29,  7, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.encoded_POS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "ds = EntityDataset(texts=sentences, pos=pos, tags=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'input_ids': tensor([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "          1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "          3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'target_pos': tensor([ 0, 19, 10, 19, 35, 34, 10, 17, 29, 31,  7, 16, 10, 17,  5, 31,  7, 16,\n",
       "         10, 11, 19, 10,  7, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'target_tag': tensor([ 0, 16, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16,\n",
       "         16,  3, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ds[0]['target_pos'].clone()\n",
    "v[[9, 0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 19, 10, 19, 35, 34, 10, 17, 29,  1,  7, 16, 10, 17,  5, 31,  7, 16,\n",
       "        10, 11, 19, 10,  7, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds[0]['target_pos'], ds[0]['target_pos'], average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score across n batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch size of 1 due to unsqueeze\n",
    "scores = []\n",
    "for i in range(ds[0]['target_pos'].unsqueeze(0).shape[0]):\n",
    "    score = f1_score(ds[0]['target_pos'].unsqueeze(0)[i], ds[0]['target_pos'].unsqueeze(0)[i], average='macro')\n",
    "    scores.append(score)\n",
    "score/ds[0]['target_pos'].unsqueeze(0).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044029672170375"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds[0]['target_pos'], v, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ds[0]['target_pos'], ds[0]['target_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ds[0]['target_pos'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bert4NER.model.model as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeller = model.EntityModel(len(le_tag.classes_), len(le_pos.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = modeller(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124, 17])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].argmax(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  4,  1, 15,  6, 15, 13, 15,  7, 15, 15, 15,  7, 15,  9, 15, 14,\n",
       "         13,  2, 15, 15, 15, 15,  7, 15,  4, 11,  9,  6, 11,  2,  9, 11, 11, 11,\n",
       "         11, 11,  7, 11,  1,  9, 11,  6,  9, 11,  9,  9, 11, 15, 15, 11,  5, 11,\n",
       "          6, 13, 11,  4, 11, 11, 11,  9, 11, 11,  0, 16, 11, 11,  9, 11,  6,  9,\n",
       "         13, 11, 11, 11, 15,  1, 15,  4, 10,  4,  4, 10,  4, 11, 11, 13, 11, 11,\n",
       "         11, 11, 11,  1, 11, 10, 15, 11,  1, 11,  8,  9, 11,  4, 11,  9, 15,  2,\n",
       "         15,  4,  4, 13,  4, 11, 11, 11, 11,  2,  7,  5, 11, 10,  5, 11]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8333, -0.1145, -0.3688,  ...,  0.2512, -0.0550,  0.3903],\n",
       "         [-0.1125,  0.2988, -0.0009,  ..., -0.1386,  0.2160,  0.3472],\n",
       "         [-0.0033,  0.3756, -0.1734,  ..., -0.3377,  0.1118,  0.5452],\n",
       "         ...,\n",
       "         [-0.3518, -0.1361,  0.0036,  ..., -0.1492,  0.2902,  0.0686],\n",
       "         [ 0.0101, -0.1048,  0.0638,  ...,  0.0149, -0.0476, -0.2547],\n",
       "         [-0.3496,  0.1182, -0.2184,  ...,  0.0937,  0.2976,  0.2573]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1181, 0.0458, 0.0355,  ..., 0.0660, 0.0486, 0.0758],\n",
       "         [0.0472, 0.0713, 0.0528,  ..., 0.0460, 0.0656, 0.0748],\n",
       "         [0.0515, 0.0753, 0.0435,  ..., 0.0369, 0.0578, 0.0892],\n",
       "         ...,\n",
       "         [0.0433, 0.0537, 0.0617,  ..., 0.0530, 0.0822, 0.0659],\n",
       "         [0.0620, 0.0553, 0.0655,  ..., 0.0623, 0.0586, 0.0476],\n",
       "         [0.0385, 0.0614, 0.0438,  ..., 0.0599, 0.0735, 0.0706]]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  4,  1, 15,  6, 15, 13, 15,  7, 15, 15, 15,  7, 15,  9, 15, 14,\n",
       "         13,  2, 15, 15, 15, 15,  7, 15,  4, 11,  9,  6, 11,  2,  9, 11, 11, 11,\n",
       "         11, 11,  7, 11,  1,  9, 11,  6,  9, 11,  9,  9, 11, 15, 15, 11,  5, 11,\n",
       "          6, 13, 11,  4, 11, 11, 11,  9, 11, 11,  0, 16, 11, 11,  9, 11,  6,  9,\n",
       "         13, 11, 11, 11, 15,  1, 15,  4, 10,  4,  4, 10,  4, 11, 11, 13, 11, 11,\n",
       "         11, 11, 11,  1, 11, 10, 15, 11,  1, 11,  8,  9, 11,  4, 11,  9, 15,  2,\n",
       "         15,  4,  4, 13,  4, 11, 11, 11, 11,  2,  7,  5, 11, 10,  5, 11]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].softmax(dim=2).argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "          1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "          3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "         1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "         3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "         1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "         3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][0] * batch['attention_mask'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
