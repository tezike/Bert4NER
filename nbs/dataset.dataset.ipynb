{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Bert4NER.config as config\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EntityDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, pos, tags, le_pos, le_tags):\n",
    "        self.texts = texts\n",
    "        self.pos = pos\n",
    "        self.tags = tags\n",
    "        self.le_tags = le_tags\n",
    "        self.le_pos = le_pos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        pos = self.pos[item]\n",
    "        tag = self.tags[item]\n",
    "\n",
    "        tokens = []\n",
    "        target_pos = []\n",
    "        target_tag = []\n",
    "        \n",
    "        # tokenize the each word in the text string\n",
    "        for i, word in enumerate(text):\n",
    "            inputs = config.TOKENIZER.encode(\n",
    "                word,\n",
    "                add_special_tokens = False,\n",
    "                truncation = True\n",
    "            )\n",
    "\n",
    "            input_len = len(inputs)\n",
    "            tokens.extend(inputs)\n",
    "\n",
    "            # the tag for that particular word should be the same for all the \n",
    "            # sub tokens of the word\n",
    "            \n",
    "            target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tag[i]] * input_len)\n",
    "\n",
    "        tokens = tokens[:config.MAX_SEQ_LEN - 2]\n",
    "        target_pos = target_pos[:config.MAX_SEQ_LEN - 2]\n",
    "        target_tag = target_tag[:config.MAX_SEQ_LEN - 2]\n",
    "\n",
    "        tokens = [101] + tokens + [102]\n",
    "        target_pos = [0] + target_pos + [0]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(tokens)\n",
    "        token_type_ids = [0] * len(tokens)\n",
    "\n",
    "        pad_len = (config.MAX_SEQ_LEN) - len(tokens)\n",
    "\n",
    "        tokens = tokens + ([0] * pad_len)\n",
    "        mask = mask + ([0] * pad_len)\n",
    "        token_type_ids = token_type_ids + ([0] * pad_len)\n",
    "        target_pos = target_pos + ([0] * pad_len)\n",
    "        target_tag = target_tag + ([0] * pad_len)\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'target_pos': torch.tensor(target_pos, dtype=torch.long),\n",
    "            'target_tag': torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "df = pd.read_csv(config.DATA_PATH/'ner_datasetreference.csv', encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the foward fill method in pandas to fill all the nans for the each sentence in the `Sentence #` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              Sentence: 1\n",
       "1              Sentence: 1\n",
       "2              Sentence: 1\n",
       "3              Sentence: 1\n",
       "4              Sentence: 1\n",
       "                ...       \n",
       "1048570    Sentence: 47959\n",
       "1048571    Sentence: 47959\n",
       "1048572    Sentence: 47959\n",
       "1048573    Sentence: 47959\n",
       "1048574    Sentence: 47959\n",
       "Name: Sentence #, Length: 1048575, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "df['Sentence #'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df['Sentence #'] = df['Sentence #'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total we cans ee that there are 47959 sentences in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47959"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['Sentence #'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us encode all the labels for every word in every sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "le_pos = LabelEncoder()\n",
    "le_tag = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "df[\"encoded_POS\"] = le_pos.fit_transform(df.POS)\n",
    "df[\"encoded_Tag\"] = le_tag.fit_transform(df.Tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['$', ',', '.', ':', ';', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ',\n",
       "        'JJR', 'JJS', 'LRB', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT',\n",
       "        'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'RRB', 'TO', 'UH',\n",
       "        'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB',\n",
       "        '``'], dtype=object),\n",
       " array(['B-art', 'B-eve', 'B-geo', 'B-gpe', 'B-nat', 'B-org', 'B-per',\n",
       "        'B-tim', 'I-art', 'I-eve', 'I-geo', 'I-gpe', 'I-nat', 'I-org',\n",
       "        'I-per', 'I-tim', 'O'], dtype=object))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "le_pos.classes_, le_tag.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now group the df according to the `Sentence #` and use it to curate the `Word`, `POS` and `Tag`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_group = df.groupby('Sentence #')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "grouped_words = sentence_group['Word'].apply(list)\n",
    "grouped_POS = sentence_group['encoded_POS'].apply(list)\n",
    "grouped_Tag = sentence_group['encoded_Tag'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence #\n",
       "Sentence: 1        [Thousands, of, demonstrators, have, marched, ...\n",
       "Sentence: 10       [Iranian, officials, say, they, expect, to, ge...\n",
       "Sentence: 100      [Helicopter, gunships, Saturday, pounded, mili...\n",
       "Sentence: 1000     [They, left, after, a, tense, hour-long, stand...\n",
       "Sentence: 10000    [U.N., relief, coordinator, Jan, Egeland, said...\n",
       "                                         ...                        \n",
       "Sentence: 9995     [Opposition, leader, Mir, Hossein, Mousavi, ha...\n",
       "Sentence: 9996     [On, Thursday, ,, Iranian, state, media, publi...\n",
       "Sentence: 9997     [Following, Iran, 's, disputed, June, 12, elec...\n",
       "Sentence: 9998     [Since, then, ,, authorities, have, held, publ...\n",
       "Sentence: 9999     [The, United, Nations, is, praising, the, use,...\n",
       "Name: Word, Length: 47959, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "grouped_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = grouped_words.values\n",
    "tags = grouped_Tag.values\n",
    "pos = grouped_POS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 47959, 47959)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "len(sentences), len(tags), len(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's construct the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 10, 19, ..., 29,  7, 16])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.encoded_POS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "ds = EntityDataset(texts=sentences, pos=pos, tags=tags, le_pos=le_pos, le_tags=le_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'input_ids': tensor([  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "          1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "          3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'target_pos': tensor([ 0, 19, 10, 19, 35, 34, 10, 17, 29, 31,  7, 16, 10, 17,  5, 31,  7, 16,\n",
       "         10, 11, 19, 10,  7, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'target_tag': tensor([ 0, 16, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16, 16,  2, 16, 16, 16, 16,\n",
       "         16,  3, 16, 16, 16, 16, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0])}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ds[0]['target_pos'].clone()\n",
    "v[[9, 0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 19, 10, 19, 35, 34, 10, 17, 29,  1,  7, 16, 10, 17,  5, 31,  7, 16,\n",
       "        10, 11, 19, 10,  7, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds[0]['target_pos'], ds[0]['target_pos'], average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score across n batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch size of 1 due to unsqueeze\n",
    "scores = []\n",
    "for i in range(ds[0]['target_pos'].unsqueeze(0).shape[0]):\n",
    "    score = f1_score(ds[0]['target_pos'].unsqueeze(0)[i], ds[0]['target_pos'].unsqueeze(0)[i], average='macro')\n",
    "    scores.append(score)\n",
    "score/ds[0]['target_pos'].unsqueeze(0).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9044029672170375"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(ds[0]['target_pos'], v, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ds[0]['target_pos'], ds[0]['target_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9838709677419355"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ds[0]['target_pos'], v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bert4NER.model.model as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeller = model.HasocModel(le_pos.classes_, le_pos.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = modeller(batch['input_ids'], batch['attention_mask'], batch['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124, 42])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].argmax(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 33, 31,  5, 16, 37, 16, 16, 35,  3,  3,  2, 35, 35, 25, 12, 25, 16,\n",
       "         16, 12, 21, 16, 22, 16,  2, 33,  3,  3,  4,  3,  3, 37, 37, 37, 37, 37,\n",
       "         39, 37, 37,  3, 12,  3, 33, 31, 37,  6, 25,  6, 16, 19, 12, 12, 32, 12,\n",
       "         31,  3, 37,  3,  3,  9,  3, 31, 37, 29,  3, 37,  3, 25, 37, 37,  3,  3,\n",
       "         37,  6,  3, 37, 37, 16, 12, 31, 12, 31, 37,  3,  4,  3,  3, 37,  3, 12,\n",
       "         12, 37, 37, 37, 25,  4,  3, 12, 29,  6, 37, 37, 37, 16, 12, 37,  2, 32,\n",
       "         12, 29, 16,  3, 16,  3, 36,  4, 31, 27,  3,  3,  3,  3,  3,  4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4539,  0.0314, -0.1590,  ...,  0.0065,  0.4704, -0.3819],\n",
       "         [-0.3163,  0.5107, -0.6074,  ...,  0.1804, -0.2988, -0.0019],\n",
       "         [-0.5365,  0.0143, -0.1526,  ..., -0.1443, -0.0552,  0.3893],\n",
       "         ...,\n",
       "         [-0.1756, -0.1651, -0.0453,  ...,  0.1317,  0.3725, -0.3099],\n",
       "         [-0.2844, -0.1919, -0.0015,  ..., -0.1084,  0.1476, -0.3613],\n",
       "         [-0.2135,  0.2744, -0.1936,  ..., -0.4689, -0.1695,  0.0111]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0142, 0.0231, 0.0191,  ..., 0.0226, 0.0359, 0.0153],\n",
       "         [0.0167, 0.0381, 0.0125,  ..., 0.0274, 0.0170, 0.0228],\n",
       "         [0.0126, 0.0219, 0.0186,  ..., 0.0187, 0.0205, 0.0319],\n",
       "         ...,\n",
       "         [0.0199, 0.0201, 0.0227,  ..., 0.0271, 0.0345, 0.0174],\n",
       "         [0.0187, 0.0205, 0.0248,  ..., 0.0223, 0.0288, 0.0173],\n",
       "         [0.0177, 0.0289, 0.0181,  ..., 0.0137, 0.0185, 0.0222]]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].softmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 33, 31,  5, 16, 37, 16, 16, 35,  3,  3,  2, 35, 35, 25, 12, 25, 16,\n",
       "         16, 12, 21, 16, 22, 16,  2, 33,  3,  3,  4,  3,  3, 37, 37, 37, 37, 37,\n",
       "         39, 37, 37,  3, 12,  3, 33, 31, 37,  6, 25,  6, 16, 19, 12, 12, 32, 12,\n",
       "         31,  3, 37,  3,  3,  9,  3, 31, 37, 29,  3, 37,  3, 25, 37, 37,  3,  3,\n",
       "         37,  6,  3, 37, 37, 16, 12, 31, 12, 31, 37,  3,  4,  3,  3, 37,  3, 12,\n",
       "         12, 37, 37, 37, 25,  4,  3, 12, 29,  6, 37, 37, 37, 16, 12, 37,  2, 32,\n",
       "         12, 29, 16,  3, 16,  3, 36,  4, 31, 27,  3,  3,  3,  3,  3,  4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].softmax(dim=2).argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 124])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  5190,  1997, 28337,  2031,  9847,  2083,  2414,  2000,  6186,\n",
       "          1996,  2162,  1999,  5712,  1998,  5157,  1996, 10534,  1997,  2329,\n",
       "          3629,  2013,  2008,  2406,  1012,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
