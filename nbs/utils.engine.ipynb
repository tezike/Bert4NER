{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Fitter(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def log(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def validate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertFitter(Fitter):\n",
    "    def __init__(self, model, dataloaders, optimizer, metrics, device, log_file='training_log.txt',scheduler=None, trial=None):\n",
    "        self.model = model\n",
    "        self.train_dl, self.valid_dl = dataloaders[0], dataloaders[1]\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        if not os.path.exists(os.path.join('..', 'outputs')): os.makedirs(os.path.join('..', 'outputs'))\n",
    "        if os.path.exists(os.path.join('..', 'outputs', f'{log_file}')): \n",
    "            os.remove(os.path.join('..', 'outputs', f'{log_file}'))\n",
    "        self.log_file = os.path.join('..', 'outputs', f'{log_file}')\n",
    "        if not isinstance(metrics, (list, tuple)): \n",
    "            metrics = list(metrics)\n",
    "        self.metrics = metrics\n",
    "        self.device = device\n",
    "        self.trial = trial #for optuna\n",
    "        \n",
    "    def fit(self, epochs, return_metric=False, monitor='epoch train_loss valid_loss metric1, metric2 time', model_path=os.path.join('..', 'weights', 'model.pth'), show_graph=True):\n",
    "        self.model_path = model_path\n",
    "        self.log(f'{time.ctime()}')\n",
    "        self.log(f'Using device: {self.device}')\n",
    "        mb = master_bar(range(1, epochs+1)) #MAJOR\n",
    "        mb.write(monitor.split(),table=True)\n",
    "        \n",
    "        model = self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "        best_metric = -np.inf\n",
    "        train_loss, valid_loss, valid_metric_0, valid_metric_1 = 0, 0, 0, 0\n",
    "        train_loss_list, valid_loss_list = [], []\n",
    "        \n",
    "        for i_, epoch in enumerate(mb):\n",
    "            epoch_start = timeit.default_timer()\n",
    "            start = time.time()\n",
    "            self.log('-'*50)\n",
    "            self.log(f'Running Epoch #{epoch} {\"ðŸ”¥\"*epoch}')\n",
    "            self.log(f'{\"-\"*50} \\n')\n",
    "            self.log('TRAINING...')\n",
    "            for ind, batch in enumerate(progress_bar(self.train_dl, parent=mb)):\n",
    "                train_loss += self.train(batch, model, optimizer, self.device, self.scheduler)\n",
    "                if ind % 500 == 0:\n",
    "                    self.log(f'Batch: {ind}, Train loss: {train_loss/ len(self.train_dl)}')\n",
    "                break\n",
    "                mb.child.comment = f'{train_loss / (ind+1 * self.train_dl.batch_size):.3f}'\n",
    "            train_loss /= mb.child.total\n",
    "            train_loss_list.append(train_loss) #for graph\n",
    "            self.log(f'Training time: {round(time.time()-start, 2)} secs \\n')\n",
    "            \n",
    "            start = time.time()\n",
    "            self.log('EVALUATING...')\n",
    "            with torch.no_grad():\n",
    "                for ind, batch in enumerate(progress_bar(self.valid_dl, parent=mb)):\n",
    "                    valid_loss_, valid_metric_ = self.validate(batch, model, self.device)\n",
    "                    valid_loss += valid_loss_\n",
    "                    valid_metric_0 += valid_metric_[0]\n",
    "                    valid_metric_1 += valid_metric_[1]\n",
    "                    if ind % 500 == 0:\n",
    "                        self.log(f'Batch: {ind}, Valid loss: {valid_loss/ len(self.valid_dl)}')\n",
    "                    break   \n",
    "                    mb.child.comment = f'{valid_loss / (ind+1 * self.train_dl.batch_size):.3f}'\n",
    "                \n",
    "                valid_loss /= mb.child.total\n",
    "                valid_metric_0 /= mb.child.total\n",
    "                valid_metric_1 /= mb.child.total\n",
    "                valid_loss_list.append(valid_loss) #for graph\n",
    "            \n",
    "            if valid_metric_1 > best_metric: #ie (f1_score > inf)\n",
    "                #             save model\n",
    "                if self.model_path is not None:\n",
    "                    if not os.path.exists(os.path.join('..', 'weights')): os.makedirs(os.path.join('..', 'weights'))\n",
    "                    self.log(f'Saving model weights at {self.model_path}')\n",
    "                    torch.save(model.state_dict(), self.model_path)\n",
    "                best_metric = valid_metric_1\n",
    "                    \n",
    "            if self.trial is not None:\n",
    "                self.trial.report(best_metric, epoch)\n",
    "\n",
    "                # Handle pruning based on the intermediate value.\n",
    "                if self.trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            if show_graph:\n",
    "                self.plot_loss_update(epoch, epochs, mb, train_loss_list, valid_loss_list) # for graph\n",
    "                               \n",
    "            epoch_end = timeit.default_timer()\n",
    "            total_time = epoch_end - epoch_start\n",
    "            mins, secs = divmod(total_time, 60)\n",
    "            hours, mins = divmod(mins, 60)\n",
    "            ret_time = f'{int(hours)}:{int(mins)}:{int(secs)}'\n",
    "            mb.write([epoch,f'{train_loss:.6f}',f'{valid_loss:.6f}',f'{valid_metric_0:.6f}', f'{valid_metric_1:.6f}', f'{ret_time}'],table=True)\n",
    "            self.log(f'Evaluation time: {ret_time}\\n')\n",
    "            break\n",
    "            \n",
    "        if return_metric: return best_metric\n",
    "    \n",
    "    def train(self, xy, model, opt, device, sched=None):\n",
    "        model.train()\n",
    "        y_tag = xy.pop('target_tag')\n",
    "        y_pos = xy.pop('target_pos')\n",
    "        x = xy\n",
    "        inputs, target_tag, target_pos = [x_.to(device) for x_ in x.values()], y_tag.to(device), y_pos.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(*inputs)\n",
    "        loss_tag = self.loss_func(out[0], target_tag, x['mask'], model.num_tag)\n",
    "        loss_pos = self.loss_func(out[1], target_pos, x['mask'], model.num_pos)\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "        loss.backward()\n",
    "        opt.step()       \n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self, xy, model, device):\n",
    "        model.eval()\n",
    "        y_tag = xy.pop('target_tag')\n",
    "        y_pos = xy.pop('target_pos')\n",
    "        x = xy\n",
    "        inputs, target_tag, target_pos = [x_.to(device) for x_ in x.values()], y_tag.to(device), y_pos.to(device)\n",
    "        out = model(*inputs)\n",
    "        loss_tag = self.loss_func(out[0], target_tag, x['mask'], model.num_tag)\n",
    "        loss_pos = self.loss_func(out[1], target_pos, x['mask'], model.num_pos)\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "        \n",
    "#         skelarn metrics to be calculated for every item in batch\n",
    "        cleaned_out_0 = out[0].cpu().softmax(2).argmax(dim=2) #[bs, seq_len, hidden_dim(num_labels)] -> [bs, seq_len]\n",
    "        cleaned_out_1 = out[1].cpu().softmax(2).argmax(dim=2) #[bs, seq_len, hidden_dim(num_labels)] -> [bs, seq_len]\n",
    "        all_metric_0, all_metric_2 = [], []\n",
    "        for i in range(target_tag.shape[0])\n",
    "            metric_0 = self.metrics[0](target_tag.cpu()[i], cleaned_out_0[i])  #sklearn metrics are (targ, inp)\n",
    "            all_metric_0.append(metric_0)\n",
    "            metric_1 = self.metrics[1](target_pos.cpu()[i], cleaned_out_1[i])  #sklearn metrics are (targ, inp)\n",
    "            all_metric_1.append(metric_1)\n",
    "            \n",
    "        metrics = list((all_metric_0 / target_tag.shape[0]),\n",
    "                   (all_metric_1 / target_tag.shape[1]))\n",
    "        return loss.item(), metrics\n",
    "            \n",
    "    def log(self, message, verbose=False):\n",
    "        if verbose: print(message)\n",
    "        with open(self.log_file, 'a+') as logger_:\n",
    "            logger_.write(f'{message}\\n')\n",
    "           \n",
    "    @staticmethod\n",
    "    def loss_func(out, target, mask, num_labels, func=nn.CrossEntropyLoss()):\n",
    "        '''loss func for NER tasks\n",
    "            out is logit from the model. Shape (bs, seq_len, hidden_dim[num_labels])\n",
    "            target is target from dataloader. Shape (bs, seq_len)\n",
    "        ''''\n",
    "        #the mask tell us where non zero tokens are\n",
    "        #the num_labels is used to tell us how many labels(le.classes_) are in the targ\n",
    "        non_zero_tokens = mask.view(-1) == 1 # zeroed token_ids have a mask of 1\n",
    "        ignore_index = func.ignore_index\n",
    "\n",
    "    #     if the token is not zero, select the corresponding target else set ignore_index\n",
    "        cleaned_target = torch.where(non_zero_tokens, target.view(-1), ignore_index) #[bs*seq_len]\n",
    "        \n",
    "        cleaned_out = out.view(-1, num_labels) #[bs*seq_len, num_labels]\n",
    "        \n",
    "        loss = func(cleaned_out, cleaned_target, ignore_index=ignore_index)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "            expects epoch to start from 1.\n",
    "        \"\"\"\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(np.array(graphs), np.array(x_bounds), np.array(y_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
