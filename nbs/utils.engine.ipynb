{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "import timeit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Fitter(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def log(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def validate(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertFitterSimple(Fitter):\n",
    "    #a simplified bertfitter. However, the model must calculate the loss so use model.EntityModelWithLoss\n",
    "    def __init__(self, model, dataloaders, optimizer, metrics, device, log_file='training_log.txt',scheduler=None, trial=None):\n",
    "        self.model = model\n",
    "        self.train_dl, self.valid_dl = dataloaders[0], dataloaders[1]\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        if not os.path.exists(os.path.join('..', 'outputs')): os.makedirs(os.path.join('..', 'outputs'))\n",
    "        if os.path.exists(os.path.join('..', 'outputs', f'{log_file}')): \n",
    "            os.remove(os.path.join('..', 'outputs', f'{log_file}'))\n",
    "        self.log_file = os.path.join('..', 'outputs', f'{log_file}')\n",
    "        if not isinstance(metrics, (list, tuple)): \n",
    "            metrics = list(metrics)\n",
    "        self.metrics = metrics\n",
    "        self.device = device\n",
    "        self.trial = trial #for optuna\n",
    "        \n",
    "    def fit(self, epochs, return_metric=False, \n",
    "            monitor='epoch train_loss valid_loss time', \n",
    "            model_path=os.path.join('..', 'weights', 'model.pth'), show_graph=True):\n",
    "        self.model_path = model_path\n",
    "        self.log(f'{time.ctime()}')\n",
    "        self.log(f'Using device: {self.device}')\n",
    "        mb = master_bar(range(1, epochs+1)) #MAJOR\n",
    "        mb.write(monitor.split(),table=True)\n",
    "        \n",
    "        model = self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "        scheduler = self.scheduler\n",
    "        best_metric = -np.inf\n",
    "        train_loss_list, valid_loss_list = [], []\n",
    "        \n",
    "        for i_, epoch in enumerate(mb):\n",
    "            epoch_start = timeit.default_timer()\n",
    "            start = time.time()\n",
    "            self.log('-'*50)\n",
    "            self.log(f'Running Epoch #{epoch} {\"ðŸ”¥\"*epoch}')\n",
    "            self.log(f'{\"-\"*50} \\n')\n",
    "            \n",
    "            self.log('TRAINING...')\n",
    "            train_loss = self.train(mb, model, optimizer, self.device, scheduler)\n",
    "            train_loss_list.append(train_loss) #for graph\n",
    "            self.log(f'Training time: {round(time.time()-start, 2)} secs \\n')\n",
    "            \n",
    "            start = time.time()\n",
    "            self.log('EVALUATING...')\n",
    "            valid_loss = self.validate(mb, model, self.device)    \n",
    "            valid_loss_list.append(valid_loss) #for graph\n",
    "            \n",
    "            if show_graph:\n",
    "                self.plot_loss_update(epoch, epochs, mb, train_loss_list, valid_loss_list) # for graph\n",
    "                               \n",
    "            epoch_end = timeit.default_timer()\n",
    "            total_time = epoch_end - epoch_start\n",
    "            mins, secs = divmod(total_time, 60)\n",
    "            hours, mins = divmod(mins, 60)\n",
    "            ret_time = f'{int(hours)}:{int(mins)}:{int(secs)}'\n",
    "            mb.write([epoch,f'{train_loss:.6f}',f'{valid_loss:.6f}',\n",
    "                      f'{ret_time}'],table=True)\n",
    "            self.log(f'Evaluation time: {ret_time}\\n')\n",
    "#             break\n",
    "            \n",
    "        if return_metric: return best_metric\n",
    "    \n",
    "    def train(self, mb, model, opt, device, sched=None):     \n",
    "        model.train()\n",
    "        train_loss = 0       \n",
    "        for ind, xy in enumerate(progress_bar(self.train_dl, parent=mb)):\n",
    "            y_tag = xy.pop('target_tag')\n",
    "            y_pos = xy.pop('target_pos')\n",
    "            x = xy\n",
    "            inputs, target_tag, target_pos = {key: x_.to(device) for key, x_ in x.items()}, y_tag.to(device), y_pos.to(device)\n",
    "            opt.zero_grad()\n",
    "            *out, loss = model(**inputs, target_tag=y_tag, target_pos=y_pos)\n",
    "            loss.backward()\n",
    "            opt.step()       \n",
    "            if sched is not None:\n",
    "                sched.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if ind % 500 == 0:\n",
    "                self.log(f'Batch: {ind}, Train loss: {train_loss/ len(self.train_dl)}')\n",
    "            \n",
    "            mb.child.comment = f'{train_loss / (ind+1) :.3f}'\n",
    "        return train_loss / len(self.train_dl)\n",
    "    \n",
    "    def validate(self, mb, model, device):    \n",
    "        model.eval()\n",
    "        valid_loss = 0      \n",
    "        with torch.no_grad():\n",
    "            for ind, xy in enumerate(progress_bar(self.valid_dl, parent=mb)):\n",
    "                y_tag = xy.pop('target_tag')\n",
    "                y_pos = xy.pop('target_pos')\n",
    "                x = xy\n",
    "                inputs, target_tag, target_pos = {key: x_.to(device) for key, x_ in x.items()}, y_tag.to(device), y_pos.to(device)\n",
    "                *out, loss = model(**inputs, target_tag=y_tag, target_pos=y_pos)\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                if ind % 500 == 0:\n",
    "                    self.log(f'Batch: {ind}, Valid loss: {valid_loss / (ind+1) :.3f}')\n",
    "\n",
    "                mb.child.comment = f'{valid_loss / (ind+1) :.3f}'  \n",
    "        return valid_loss / len(self.valid_dl)\n",
    "        \n",
    "            \n",
    "    def log(self, message, verbose=False):\n",
    "        if verbose: print(message)\n",
    "        with open(self.log_file, 'a+') as logger_:\n",
    "            logger_.write(f'{message}\\n')\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "            expects epoch to start from 1.\n",
    "        \"\"\"\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(np.array(graphs), np.array(x_bounds), np.array(y_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BertFitter(Fitter):\n",
    "    def __init__(self, model, dataloaders, optimizer, metrics, device, log_file='training_log.txt',scheduler=None, trial=None):\n",
    "        self.model = model\n",
    "        self.train_dl, self.valid_dl = dataloaders[0], dataloaders[1]\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        if not os.path.exists(os.path.join('..', 'outputs')): os.makedirs(os.path.join('..', 'outputs'))\n",
    "        if os.path.exists(os.path.join('..', 'outputs', f'{log_file}')): \n",
    "            os.remove(os.path.join('..', 'outputs', f'{log_file}'))\n",
    "        self.log_file = os.path.join('..', 'outputs', f'{log_file}')\n",
    "        if not isinstance(metrics, (list, tuple)): \n",
    "            metrics = list(metrics)\n",
    "        self.metrics = metrics\n",
    "        self.device = device\n",
    "        self.trial = trial #for optuna\n",
    "        \n",
    "    def fit(self, epochs, return_metric=False, \n",
    "            monitor='epoch train_loss valid_loss tag_accuracy tag_f1_score pos_accuracy pos_f1_score time', \n",
    "            model_path=os.path.join('..', 'weights', 'model.pth'), show_graph=True):\n",
    "        self.model_path = model_path\n",
    "        self.log(f'{time.ctime()}')\n",
    "        self.log(f'Using device: {self.device}')\n",
    "        mb = master_bar(range(1, epochs+1)) #MAJOR\n",
    "        mb.write(monitor.split(),table=True)\n",
    "        \n",
    "        model = self.model.to(self.device)\n",
    "        optimizer = self.optimizer\n",
    "        best_metric = -np.inf\n",
    "        train_loss_list, valid_loss_list = [], []\n",
    "        \n",
    "        for i_, epoch in enumerate(mb):\n",
    "            epoch_start = timeit.default_timer()\n",
    "            start = time.time()\n",
    "            self.log('-'*50)\n",
    "            self.log(f'Running Epoch #{epoch} {\"ðŸ”¥\"*epoch}')\n",
    "            self.log(f'{\"-\"*50} \\n')\n",
    "            self.log('TRAINING...')\n",
    "            \n",
    "            train_loss, valid_loss = 0, 0\n",
    "            valid_metric_0_tag, valid_metric_1_tag =  0, 0\n",
    "            valid_metric_0_pos, valid_metric_1_pos =  0, 0\n",
    "            for ind, batch in enumerate(progress_bar(self.train_dl, parent=mb)):\n",
    "                train_loss += self.train(batch, model, optimizer, self.device, self.scheduler)\n",
    "                if ind % 500 == 0:\n",
    "                    self.log(f'Batch: {ind}, Train loss: {train_loss / (ind+1) :.3f}')\n",
    "#                 break\n",
    "                mb.child.comment = f'{train_loss / (ind+1) :.3f}'\n",
    "            train_loss /= mb.child.total\n",
    "        \n",
    "            train_loss_list.append(train_loss) #for graph\n",
    "            self.log(f'Training time: {round(time.time()-start, 2)} secs \\n')\n",
    "            \n",
    "            start = time.time()\n",
    "            self.log('EVALUATING...')\n",
    "            with torch.no_grad():\n",
    "                for ind, batch in enumerate(progress_bar(self.valid_dl, parent=mb)):\n",
    "                    valid_loss_, valid_metric_ = self.validate(batch, model, self.device)\n",
    "                    valid_loss += valid_loss_\n",
    "                    valid_metric_0_tag += valid_metric_[0]\n",
    "                    valid_metric_1_tag += valid_metric_[1]\n",
    "                    valid_metric_0_pos += valid_metric_[2]\n",
    "                    valid_metric_1_pos += valid_metric_[3]\n",
    "                    if ind % 500 == 0:\n",
    "                        self.log(f'Batch: {ind}, Valid loss: {valid_loss / (ind+1) :.3f}')\n",
    "#                     break   \n",
    "                    mb.child.comment = f'{valid_loss / (ind+1) :.3f}'\n",
    "    \n",
    "            valid_loss /= mb.child.total\n",
    "            valid_metric_0_tag /= mb.child.total\n",
    "            valid_metric_1_tag /= mb.child.total\n",
    "            valid_metric_0_pos /= mb.child.total\n",
    "            valid_metric_1_pos /= mb.child.total\n",
    "            valid_loss_list.append(valid_loss) #for graph\n",
    "            \n",
    "            if valid_metric_1_tag > best_metric: #ie (f1_score > inf)\n",
    "                #             save model\n",
    "                if self.model_path is not None:\n",
    "                    if not os.path.exists(os.path.join('..', 'weights')): os.makedirs(os.path.join('..', 'weights'))\n",
    "                    self.log(f'Saving model weights at {self.model_path}')\n",
    "                    torch.save(model.state_dict(), self.model_path)\n",
    "                best_metric = valid_metric_0_pos\n",
    "                    \n",
    "            if self.trial is not None:\n",
    "                self.trial.report(best_metric, epoch)\n",
    "\n",
    "                # Handle pruning based on the intermediate value.\n",
    "                if self.trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            if show_graph:\n",
    "                self.plot_loss_update(epoch, epochs, mb, train_loss_list, valid_loss_list) # for graph\n",
    "                               \n",
    "            epoch_end = timeit.default_timer()\n",
    "            total_time = epoch_end - epoch_start\n",
    "            mins, secs = divmod(total_time, 60)\n",
    "            hours, mins = divmod(mins, 60)\n",
    "            ret_time = f'{int(hours)}:{int(mins)}:{int(secs)}'\n",
    "            mb.write([epoch,f'{train_loss:.6f}',f'{valid_loss:.6f}',\n",
    "                      f'{valid_metric_0_tag:.6f}', f'{valid_metric_1_tag:.6f}', \n",
    "                      f'{valid_metric_0_pos:.6f}', f'{valid_metric_1_pos:.6f}', \n",
    "                      f'{ret_time}'],table=True)\n",
    "            self.log(f'Evaluation time: {ret_time}\\n')\n",
    "#             break\n",
    "            \n",
    "        if return_metric: return best_metric\n",
    "    \n",
    "    def train(self, xy, model, opt, device, sched=None):\n",
    "        model.train()\n",
    "        y_tag = xy.pop('target_tag')\n",
    "        y_pos = xy.pop('target_pos')\n",
    "        x = xy\n",
    "        inputs, target_tag, target_pos = [x_.to(device) for x_ in x.values()], y_tag.to(device), y_pos.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(*inputs)\n",
    "        loss_tag = self.loss_func(out[0], target_tag, x['attention_mask'], model.num_tag)\n",
    "        loss_pos = self.loss_func(out[1], target_pos, x['attention_mask'], model.num_pos)\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "        loss.backward()\n",
    "        opt.step()       \n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self, xy, model, device):\n",
    "        model.eval()\n",
    "        y_tag = xy.pop('target_tag')\n",
    "        y_pos = xy.pop('target_pos')\n",
    "        x = xy\n",
    "        inputs, target_tag, target_pos = [x_.to(device) for x_ in x.values()], y_tag.to(device), y_pos.to(device)\n",
    "        out = model(*inputs)\n",
    "        loss_tag = self.loss_func(out[0], target_tag, x['attention_mask'], model.num_tag)\n",
    "        loss_pos = self.loss_func(out[1], target_pos, x['attention_mask'], model.num_pos)\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "        \n",
    "#         skelarn metrics to be calculated for every item in batch\n",
    "        cleaned_out_tag = out[0].cpu().softmax(2).argmax(dim=2) #[bs, seq_len, hidden_dim(num_labels)] -> [bs, seq_len]\n",
    "        cleaned_out_pos = out[1].cpu().softmax(2).argmax(dim=2) #[bs, seq_len, hidden_dim(num_labels)] -> [bs, seq_len]\n",
    "        all_metric_tag, all_metric_pos = [], []\n",
    "        for i in range(target_tag.shape[0]):\n",
    "            metric_0_tag = self.process_metric(target_tag, cleaned_out_tag, self.metrics[0], x, i)\n",
    "            metric_1_tag = self.process_metric(target_tag, cleaned_out_tag, self.metrics[1], x, i)\n",
    "            all_metric_tag.append([metric_0_tag, metric_1_tag])\n",
    "\n",
    "            metric_0_pos = self.process_metric(target_pos, cleaned_out_pos, self.metrics[0], x, i)#sklearn metrics are (targ, inp)\n",
    "            metric_1_pos = self.process_metric(target_pos, cleaned_out_pos, self.metrics[1], x, i)\n",
    "            all_metric_pos.append([metric_0_pos, metric_1_pos])\n",
    "        \n",
    "        all_metric_tag = np.array(all_metric_tag)\n",
    "        all_metric_pos = np.array(all_metric_pos)\n",
    "        metrics = ((sum(all_metric_tag[:, 0]) / target_tag.shape[0]),\n",
    "                   (sum(all_metric_tag[:, 1]) / target_tag.shape[0]),\n",
    "                   (sum(all_metric_pos[:, 0]) / target_tag.shape[0]),\n",
    "                   (sum(all_metric_pos[:, 1]) / target_tag.shape[0]))\n",
    "        \n",
    "        return loss.item(), metrics\n",
    "            \n",
    "    def log(self, message, verbose=False):\n",
    "        if verbose: print(message)\n",
    "        with open(self.log_file, 'a+') as logger_:\n",
    "            logger_.write(f'{message}\\n')\n",
    "            \n",
    "    @staticmethod\n",
    "    def process_metric(target, output, metric, x, i):\n",
    "        return metric(target[i].cpu()[torch.nonzero(x['attention_mask'][i]).flatten()], \n",
    "                        output[i][torch.nonzero(x['attention_mask'][i]).flatten()])\n",
    "           \n",
    "    @staticmethod\n",
    "    def loss_func(out, target, mask, num_labels, func=nn.CrossEntropyLoss()):\n",
    "        '''loss func for NER tasks\n",
    "            out is logit from the model. Shape (bs, seq_len, hidden_dim[num_labels])\n",
    "            target is target from dataloader. Shape (bs, seq_len)\n",
    "        '''\n",
    "        #the mask tell us where non zero tokens are\n",
    "        #the num_labels is used to tell us how many labels(le.classes_) are in the targ\n",
    "        non_zero_tokens = mask.view(-1) == 1 # zeroed token_ids have a mask of 1\n",
    "        ignore_index = func.ignore_index\n",
    "\n",
    "    #     if the token is not zero, select the corresponding target else set ignore_index\n",
    "        cleaned_target = torch.where(non_zero_tokens.to(target.device), \n",
    "                                     target.view(-1), \n",
    "                                     torch.tensor(ignore_index).to(target.device)) #[bs*seq_len]\n",
    "        \n",
    "        cleaned_out = out.view(-1, num_labels) #[bs*seq_len, num_labels]\n",
    "        \n",
    "        loss = func(cleaned_out, cleaned_target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        \"\"\" dynamically print the loss plot during the training/validation loop.\n",
    "            expects epoch to start from 1.\n",
    "        \"\"\"\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(np.array(graphs), np.array(x_bounds), np.array(y_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nonzero??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
