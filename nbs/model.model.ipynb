{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class HasocModel(nn.Module):\n",
    "    def __init__(self, num_tag, num_pos, drop=0.3, model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        _config = transformers.BertConfig.from_pretrained(model_name)\n",
    "        self.model = transformers.BertModel.from_pretrained(model_name, config=_config)\n",
    "        \n",
    "        self.num_tag = num_tag\n",
    "        self.num_pos = num_pos\n",
    "        \n",
    "        self.tok = transformers.BertTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_name,\n",
    "            do_lower_case=True,\n",
    "            )\n",
    "        out_size = getattr(self.model, 'pooler').dense.out_features\n",
    "        self.drop_tag = nn.Dropout(drop)\n",
    "        self.drop_pos = nn.Dropout(drop)\n",
    "\n",
    "        self.classifier_tag = nn.Linear(out_size, self.num_tag)\n",
    "        self.classifier_pos = nn.Linear(out_size, self.num_pos)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        enc_hidden, enc_attn_mask = self.model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                           )\n",
    "#         enc_hidden: (batch_size, sequence_length, hidden_size)\n",
    "#         enc_attn_mask: (batch_size, sequence_length)\n",
    "#         We don't need the enc_attn_mask as we are not building a decoder\n",
    "        out_tag = self.drop_tag(enc_hidden)\n",
    "        out_pos = self.drop_pos(enc_hidden)\n",
    "#         each token in the sequence has `out_tag` or `out_pos` num of predictions \n",
    "#         so we need to softmax the predictions and take the max\n",
    "        return self.classifier_tag(out_tag), self.classifier_pos(out_pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
