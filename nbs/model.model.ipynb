{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag, num_pos, drop=0.3, model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        _config = transformers.BertConfig.from_pretrained(model_name)\n",
    "        self.model = transformers.BertModel.from_pretrained(model_name)#, config=_config)\n",
    "        \n",
    "        self.num_tag = num_tag\n",
    "        self.num_pos = num_pos\n",
    "        \n",
    "        out_size = getattr(self.model, 'pooler').dense.out_features\n",
    "        self.drop_tag = nn.Dropout(drop)\n",
    "        self.drop_pos = nn.Dropout(drop)\n",
    "\n",
    "        self.classifier_tag = nn.Linear(out_size, self.num_tag)\n",
    "        self.classifier_pos = nn.Linear(out_size, self.num_pos)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        enc_hidden, _ = self.model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                           )\n",
    "#         enc_hidden: (batch_size, sequence_length, hidden_size)\n",
    "        out_tag = self.drop_tag(enc_hidden)\n",
    "        out_pos = self.drop_pos(enc_hidden)\n",
    "        \n",
    "        return self.classifier_tag(out_tag), self.classifier_pos(out_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EntityModelWithLoss(nn.Module):\n",
    "    def __init__(self, num_tag, num_pos, drop=0.3, model_name='bert-base-uncased'):\n",
    "        super().__init__()\n",
    "        _config = transformers.BertConfig.from_pretrained(model_name)\n",
    "        self.model = transformers.BertModel.from_pretrained(model_name)#, config=_config)\n",
    "        \n",
    "        self.num_tag = num_tag\n",
    "        self.num_pos = num_pos\n",
    "        \n",
    "        out_size = getattr(self.model, 'pooler').dense.out_features\n",
    "        self.drop_tag = nn.Dropout(drop)\n",
    "        self.drop_pos = nn.Dropout(drop)\n",
    "\n",
    "        self.classifier_tag = nn.Linear(out_size, self.num_tag)\n",
    "        self.classifier_pos = nn.Linear(out_size, self.num_pos)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, target_tag, target_pos):\n",
    "        enc_hidden, _ = self.model(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                           )\n",
    "#         enc_hidden: (batch_size, sequence_length, hidden_size)\n",
    "        out_tag = self.drop_tag(enc_hidden)\n",
    "        out_pos = self.drop_pos(enc_hidden)\n",
    "        \n",
    "        out_tag = self.classifier_tag(out_tag)\n",
    "        out_pos = self.classifier_pos(out_pos)\n",
    "        \n",
    "        #calc loss\n",
    "        loss_tag = loss_func(out_tag, target_tag, attention_mask, self.num_tag)\n",
    "        loss_pos = loss_func(out_pos, target_pos, attention_mask, self.num_pos)\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "#         each token in the sequence has `out_tag` or `out_pos` num of predictions \n",
    "#         so we need to softmax the predictions and take the max\n",
    "#         return self.classifier_tag(out_tag), self.classifier_pos(out_pos), loss\n",
    "        return out_tag, out_pos, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def loss_func(out, target, mask, num_labels, func=nn.CrossEntropyLoss()):\n",
    "    '''loss func for NER tasks\n",
    "        out is logit from the model. Shape (bs, seq_len, hidden_dim[num_labels])\n",
    "        target is target from dataloader. Shape (bs, seq_len)\n",
    "    '''\n",
    "    #the mask tell us where non zero tokens are\n",
    "    #the num_labels is used to tell us how many labels(le.classes_) are in the targ\n",
    "    non_zero_tokens = mask.view(-1) == 1 # zeroed token_ids have a mask of 1\n",
    "    ignore_index = func.ignore_index\n",
    "\n",
    "#     if the token is not zero, select the corresponding target else set ignore_index\n",
    "    cleaned_target = torch.where(non_zero_tokens.to(target.device), \n",
    "                                 target.view(-1), \n",
    "                                 torch.tensor(ignore_index).to(target.device).type_as(target)) #[bs*seq_len]\n",
    "\n",
    "    cleaned_out = out.view(-1, num_labels) #[bs*seq_len, n++um_labels]\n",
    "\n",
    "    loss = func(cleaned_out.to(target.device), cleaned_target.to(target.device))\n",
    "\n",
    "#     loss = func(cleaned_out, target.view(-1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
